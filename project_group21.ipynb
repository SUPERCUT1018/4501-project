{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding Hired Rides in NYC\n",
    "Project prompt\n",
    "\n",
    "This scaffolding notebook may be used to help setup your final project. It's totally optional whether you make use of this or not.\n",
    "\n",
    "If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish.\n",
    "\n",
    "Anything in italics (prose) or comments (in code) is meant to provide you with guidance. Remove the italic lines and provided comments before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading.\n",
    "\n",
    "All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:// 1. bonus: Test needed; [NOT DONE]\n",
    "# 2. table type and primary key should be checked; [DONE]\n",
    "# 3. docuement need to be enhanced. [DONE]\n",
    "# 4. the way download the data?  [DDNE]\n",
    "# 5.check the correctness of value;  [DONE]\n",
    "# 6.bonus: sunset table and one more vis? [NOT DONE]\n",
    "\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from tqdm import tqdm \n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.sql import text\n",
    "from ipywidgets import interact\n",
    "from scipy.stats import sem, t\n",
    "from ipywidgets import SelectMultiple\n",
    "from folium.plugins import HeatMap\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import ipywidgets as widgets\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import re \n",
    "import os\n",
    "import folium\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TLC_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "WEATHER_CSV_DIR = \"./weather_data\"\n",
    "\n",
    "PARQUET_DIR = \"parquet_files\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\"\n",
    "\n",
    "engine = create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones(shapefile:str='./taxi_zones.shp') -> gpd.GeoDataFrame :\n",
    "    \"\"\"\n",
    "    Load the taxi zone shapefile into a GeoDataFrame\n",
    "\n",
    "    Keyword Arguments:\n",
    "    shapefile {str} -- the path to the shapefile (default: {'./taxi_zones.shp'})\n",
    "\n",
    "    Returns:\n",
    "    gpd.GeoDataFrame -- the GeoDataFrame containing the taxi zones\n",
    "    \"\"\"\n",
    "    gdf = gpd.read_file(shapefile).to_crs(4326)\n",
    "    gdf['latitude'] = gdf.geometry.centroid.y\n",
    "    gdf['longitude'] = gdf.geometry.centroid.x\n",
    "    gdf = gdf[['LocationID', 'latitude', 'longitude']]\n",
    "    return gdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "DataSourceError",
     "evalue": "./taxi_zones.shp: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDataSourceError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34660\\390811571.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_taxi_zones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34660\\2327627785.py\u001b[0m in \u001b[0;36mload_taxi_zones\u001b[1;34m(shapefile)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mgpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGeoDataFrame\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mGeoDataFrame\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtaxi\u001b[0m \u001b[0mzones\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \"\"\"\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mgdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapefile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_crs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4326\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mgdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'latitude'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcentroid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mgdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'longitude'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcentroid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\py3\\lib\\site-packages\\geopandas\\io\\file.py\u001b[0m in \u001b[0;36m_read_file\u001b[1;34m(filename, bbox, mask, columns, rows, engine, **kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"pyogrio\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m         return _read_file_pyogrio(\n\u001b[0m\u001b[0;32m    295\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         )\n",
      "\u001b[1;32md:\\py3\\lib\\site-packages\\geopandas\\io\\file.py\u001b[0m in \u001b[0;36m_read_file_pyogrio\u001b[1;34m(path_or_bytes, bbox, mask, rows, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"columns\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"include_fields\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mpyogrio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_bytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\py3\\lib\\site-packages\\pyogrio\\geopandas.py\u001b[0m in \u001b[0;36mread_dataframe\u001b[1;34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, fid_as_index, use_arrow, on_invalid, arrow_to_pandas_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[1;31m# as numpy does not directly support timezones.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"datetime_as_string\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m     result = read_func(\n\u001b[0m\u001b[0;32m    266\u001b[0m         \u001b[0mpath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[0mlayer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\py3\\lib\\site-packages\\pyogrio\\raw.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, return_fids, datetime_as_string, **kwargs)\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[0mdataset_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_preprocess_options_key_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m     return ogr_read(\n\u001b[0m\u001b[0;32m    199\u001b[0m         \u001b[0mget_vsi_path_or_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[0mlayer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpyogrio\\\\_io.pyx\u001b[0m in \u001b[0;36mpyogrio._io.ogr_read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpyogrio\\\\_io.pyx\u001b[0m in \u001b[0;36mpyogrio._io.ogr_open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mDataSourceError\u001b[0m: ./taxi_zones.shp: No such file or directory"
     ]
    }
   ],
   "source": [
    "gdf = load_taxi_zones()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sample_size(population,z=1.96, e=0.05) -> int:\n",
    "    \"\"\"\n",
    "    calculate the sample size needed for a given population\n",
    "    :param population: the size of the population\n",
    "    :param z: the parameter of confidence level\n",
    "    :param e: the margin of error\n",
    "\n",
    "    :return: the sample size needed\n",
    "    \"\"\"\n",
    "    n0 = z**2 * 0.5 * 0.5 / e**2\n",
    "    n = n0 / (1 + (n0 - 1) / population)\n",
    "    return int(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_location_id_to_lat_lon(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    convert the pickup and dropoff location id to latitude and longitude\n",
    "\n",
    "    Keyword arguments:\n",
    "    df -- the dataframe to be converted\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame -- the dataframe with latitude and longitude columns\n",
    "    \"\"\"\n",
    "    df = df.merge(gdf, left_on='PULocationID', right_on='LocationID', how='left').rename(columns={'latitude':'PULatitude', 'longitude':'PULongitude'}).drop(columns='LocationID')\n",
    "    df = df.dropna(subset=['PULatitude', 'PULongitude'])\n",
    "    df = df.merge(gdf, left_on='DOLocationID', right_on='LocationID', how='left').rename(columns={'latitude':'DOLatitude', 'longitude':'DOLongitude'}).drop(columns='LocationID')\n",
    "    df = df.dropna(subset=['DOLatitude', 'DOLongitude'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_taxi_urls(all_urls:List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    filter the urls that contain the yellow taxi data\n",
    "\n",
    "    Keyword arguments:\n",
    "    all_urls -- the list of urls to be filtered\n",
    "\n",
    "    Returns:\n",
    "    List[str] -- the list of filtered urls\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'(yellow)_tripdata_.*?(202[0-4])-([0-1][0-9])')\n",
    "    result = []\n",
    "    for url in all_urls:\n",
    "        filename = url.split(\"/\")[-1]\n",
    "        match = pattern.match(filename)\n",
    "        if match is not None:\n",
    "            result.append(url)\n",
    "    return result\n",
    "\n",
    "\n",
    "def filter_urls(all_urls:List[str],re_pattern :str= '(fhvhv)_tripdata_.*?(202[0-4])-([0-1][0-9])') -> List[str]:\n",
    "    \"\"\"\n",
    "    filter url with given pattern\n",
    "\n",
    "    Keyword arguments:\n",
    "    all_urls -- the list of urls to be filtered\n",
    "    re_pattern -- the pattern to be matched\n",
    "\n",
    "    Returns:\n",
    "    List[str] -- the list of filtered urls\n",
    "    \"\"\"\n",
    "    pattern = re.compile(f\"{re_pattern}\")\n",
    "    result = []\n",
    "    for url in all_urls:\n",
    "        filename = url.split(\"/\")[-1]\n",
    "        match = pattern.match(filename)\n",
    "        if match is not None:\n",
    "            result.append(url)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid_record_of_taxi_data(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    remove the data points outside of NYC\n",
    "    remove the data with a nan pick\n",
    "    remove the distance less equal than 0\n",
    "    \n",
    "    Keyword arguments:\n",
    "\n",
    "    Returns:\n",
    "    pd.Dataframe -- the row if it is valid, None otherwise\n",
    "    \"\"\"\n",
    "    valid = (\n",
    "            (df['PULatitude'].between(NEW_YORK_BOX_COORDS[0][0], NEW_YORK_BOX_COORDS[1][0])) &\n",
    "            (df['PULongitude'].between(NEW_YORK_BOX_COORDS[0][1], NEW_YORK_BOX_COORDS[1][1])) &\n",
    "            (df['DOLatitude'].between(NEW_YORK_BOX_COORDS[0][0], NEW_YORK_BOX_COORDS[1][0])) &\n",
    "            (df['DOLongitude'].between(NEW_YORK_BOX_COORDS[0][1], NEW_YORK_BOX_COORDS[1][1])) &\n",
    "            (~df['tpep_pickup_datetime'].isna()) &\n",
    "            (~df['tpep_dropoff_datetime'].isna()) &\n",
    "            (df['trip_distance'] > 0)\n",
    "        )\n",
    "    \n",
    "    return df[valid]\n",
    "\n",
    "def get_and_clean_month(url:str)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    download and clean the data from the given url\n",
    "\n",
    "    Keyword arguements:\n",
    "    url -- the url to download the data\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame -- the cleaned dataframe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parquet_file = f\"{url.split('/')[-1].strip()}\"\n",
    "        if os.path.exists(f\"{PARQUET_DIR}/{parquet_file}\"):\n",
    "            df = pd.read_parquet(f\"{PARQUET_DIR}/{parquet_file}\")\n",
    "        else:\n",
    "            # wget = f\"wget {url.strip()} -O {PARQUET_DIR}/{parquet_file}\"\n",
    "            # os.system(wget)\n",
    "\n",
    "            response = requests.get(url.strip(), stream=True)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "            file_path = os.path.join(PARQUET_DIR, parquet_file)\n",
    "            # Save the content to the file\n",
    "            with open(file_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            df = pd.read_parquet(f\"{PARQUET_DIR}/{parquet_file}\")\n",
    "            \n",
    "        n = calculate_sample_size(df.shape[0])\n",
    "        df = df.sample(n)\n",
    "        df = convert_location_id_to_lat_lon(df)\n",
    "        df = remove_invalid_record_of_taxi_data(df)\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(parquet_urls:List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    download and clean the data from the given urls\n",
    "\n",
    "    Keyword arguements:\n",
    "    parquet_urls -- the list of urls to download the data\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame -- the cleaned dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    all_taxi_dataframes = []\n",
    "    parquet_urls = filter_urls(parquet_urls,'(yellow)_tripdata_.*?(202[0-4])-([0-1][0-9])')\n",
    "    for parquet_url in tqdm(parquet_urls):\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_month(parquet_url)\n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        \n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_taxi_page(url:str) -> List[str]:\n",
    "    \"\"\"\n",
    "    get all urls from the page of the given url\n",
    "\n",
    "    Keyword arguments:\n",
    "    url -- the url to get all urls from\n",
    "\n",
    "    Returns:\n",
    "    List[str] -- the list of all urls\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = bs4.BeautifulSoup(response.content, 'html.parser')\n",
    "    urls = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "    return urls\n",
    "\n",
    "\n",
    "def find_all_parquet_urls(urls:List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    filter the all parquet urls \n",
    "\n",
    "    Keyword arguments:\n",
    "    urls -- the list of urls to be filtered\n",
    "\n",
    "    Returns:\n",
    "    List[str] -- the list of filtered urls\n",
    "    \"\"\"\n",
    "    parquet_urls = [url for url in urls if 'parquet' in url]\n",
    "    return parquet_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data() ->pd.DataFrame:\n",
    "    \"\"\" \n",
    "    get the taxi data from the TLC website\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame -- the taxi data\n",
    "    \"\"\"\n",
    "    if not os.path.exists(PARQUET_DIR):\n",
    "        os.mkdir(PARQUET_DIR)\n",
    "    all_urls = get_all_urls_from_taxi_page(TLC_URL)\n",
    "    all_parquet_urls = find_all_parquet_urls(all_urls)\n",
    "    taxi_data = get_and_clean_taxi_data(all_parquet_urls)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data_uncleaned = get_taxi_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data_cleaned = taxi_data_uncleaned[['tpep_pickup_datetime','tpep_dropoff_datetime',\n",
    "                               'trip_distance','fare_amount','extra','mta_tax','improvement_surcharge',\n",
    "                               'tolls_amount',\n",
    "                               'PULatitude','PULongitude',\n",
    "                               'DOLatitude','DOLongitude']]\n",
    "\n",
    "taxi_data_cleaned['base_fare'] = taxi_data_cleaned['fare_amount'] \n",
    "taxi_data_cleaned['tax'] = taxi_data_cleaned['mta_tax'] + taxi_data_cleaned['extra']\n",
    "taxi_data_cleaned['tolls'] = taxi_data_cleaned['tolls_amount']\n",
    "taxi_data_cleaned['surcharge'] = taxi_data_cleaned['improvement_surcharge']\n",
    "taxi_data_cleaned = taxi_data_cleaned.rename(columns={'tpep_pickup_datetime':'trip_pickup_datetime',\n",
    "                                                      'tpep_dropoff_datetime':'trip_dropoff_datetime',\n",
    "                                                      'trip_distance':'trip_miles',\n",
    "                                                      'base_fare':'base_fare',\n",
    "                                                      'tax':'tax',\n",
    "                                                      'tolls':'tolls',\n",
    "                                                      'PULatitude':'pickup_latitude',\n",
    "                                                      'PULongitude':'pickup_longitude',\n",
    "                                                      'DOLatitude':'dropoff_latitude',\n",
    "                                                      'DOLongitude':'dropoff_longitude'})\n",
    "taxi_data_cleaned = taxi_data_cleaned[['trip_pickup_datetime','trip_dropoff_datetime','trip_miles','base_fare','tax','tolls','surcharge',\n",
    "                                       'pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude']]\n",
    "taxi_data = taxi_data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid_records_of_uber_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove invalid data points from the Uber dataset.\n",
    "\n",
    "    Keyword arguments:\n",
    "    df -- the DataFrame to clean\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame -- the cleaned DataFrame\n",
    "    \"\"\"\n",
    "    valid = (\n",
    "        (df['PULatitude'].between(NEW_YORK_BOX_COORDS[0][0], NEW_YORK_BOX_COORDS[1][0])) &\n",
    "        (df['PULongitude'].between(NEW_YORK_BOX_COORDS[0][1], NEW_YORK_BOX_COORDS[1][1])) &\n",
    "        (df['DOLatitude'].between(NEW_YORK_BOX_COORDS[0][0], NEW_YORK_BOX_COORDS[1][0])) &\n",
    "        (df['DOLongitude'].between(NEW_YORK_BOX_COORDS[0][1], NEW_YORK_BOX_COORDS[1][1])) &\n",
    "        (~df['pickup_datetime'].isna()) &\n",
    "        (~df['dropoff_datetime'].isna()) &\n",
    "        (df['trip_miles'] > 0)\n",
    "    )\n",
    "\n",
    "    # Return the filtered DataFrame\n",
    "    return df[valid]\n",
    "\n",
    "\n",
    "def get_and_clean_uber_month(url:List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    download and clean the data from the given url of uber\n",
    "\n",
    "    Keyword arguements:\n",
    "    url -- the url to download the data\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame -- the cleaned dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        parquet_file = f\"{url.split('/')[-1].strip()}\"\n",
    "        if os.path.exists(f\"{PARQUET_DIR}/{parquet_file}\"):\n",
    "            df = pd.read_parquet(f\"{PARQUET_DIR}/{parquet_file}\")\n",
    "        else:\n",
    "            # wget = f\"wget {url.strip()} -O {PARQUET_DIR}/{parquet_file}\"\n",
    "            # os.system(wget)\n",
    "\n",
    "            response = requests.get(url.strip(), stream=True)\n",
    "            response.raise_for_status()\n",
    "            file_path = os.path.join(PARQUET_DIR, parquet_file)\n",
    "            with open(file_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            df = pd.read_parquet(f\"{PARQUET_DIR}/{parquet_file}\")\n",
    "            \n",
    "        n = calculate_sample_size(df.shape[0])\n",
    "        df = df[df['hvfhs_license_num']=='HV0003']\n",
    "        df = df.sample(n)\n",
    "        df = convert_location_id_to_lat_lon(df)\n",
    "        df = remove_invalid_records_of_uber_data(df)\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_uber_data(parquet_urls:List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    download and clean the data from the given urls of uber\n",
    "\n",
    "    Keyword arguements:\n",
    "    parquet_urls -- the list of urls to download the data\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame -- the cleaned dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    all_uber_dataframes = []\n",
    "\n",
    "    uber_parquet_urls = filter_urls(parquet_urls, re_pattern='(fhvhv)_tripdata_.*?(202[0-4])-([0-1][0-9])')\n",
    "\n",
    "    for parquet_url in tqdm(uber_parquet_urls):\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_uber_month(parquet_url)\n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        \n",
    "        all_uber_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    uber_data = pd.concat(all_uber_dataframes)\n",
    "    return uber_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data() -> pd.DataFrame :\n",
    "    \"\"\"\n",
    "    get the uber data from the TLC website\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame -- the uber data\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(PARQUET_DIR):\n",
    "        os.mkdir(PARQUET_DIR)\n",
    "    all_urls = get_all_urls_from_taxi_page(TLC_URL)\n",
    "    all_parquet_urls = find_all_parquet_urls(all_urls)\n",
    "    taxi_data = get_and_clean_uber_data(all_parquet_urls)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data_uncleaned = get_uber_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data_cleaned = uber_data_uncleaned[['pickup_datetime','dropoff_datetime','trip_miles',\n",
    "                               'base_passenger_fare','tolls','bcf','sales_tax','congestion_surcharge',\n",
    "                               'PULatitude','PULongitude',\n",
    "                               'DOLatitude','DOLongitude']]\n",
    "\n",
    "uber_data_cleaned['base_fare'] = uber_data_cleaned['base_passenger_fare'] #+ uber_data_cleaned['bcf']\n",
    "uber_data_cleaned['surcharge'] = uber_data_cleaned['congestion_surcharge']\n",
    "uber_data_cleaned['tax'] = uber_data_cleaned['sales_tax'] +   uber_data_cleaned['bcf'] # TODO: check if this is correct\n",
    "uber_data_cleaned['tolls'] = uber_data_cleaned['tolls']\n",
    "uber_data_cleaned = uber_data_cleaned.rename(columns={\n",
    "    'trip_miles':'trip_distance',\n",
    "    'pickup_datetime':'trip_pickup_datetime',\n",
    "    'dropoff_datetime':'trip_dropoff_datetime',\n",
    "    'base_fare':'base_fare',\n",
    "    'tax':'tax',\n",
    "    'tolls':'tolls',\n",
    "    'surcharge':'surcharge',\n",
    "    'PULatitude':'pickup_latitude',\n",
    "    'PULongitude':'pickup_longitude',\n",
    "    'DOLatitude':'dropoff_latitude',\n",
    "    'DOLongitude':'dropoff_longitude'\n",
    "})\n",
    "\n",
    "uber_data_cleaned = uber_data_cleaned[['trip_pickup_datetime','trip_dropoff_datetime','trip_distance','base_fare','tax','tolls','surcharge',\n",
    "                                       'pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude']]\n",
    "\n",
    "uber_data = uber_data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory:str) -> List[str]:\n",
    "    \"\"\"\n",
    "    list all the csv files in the given directory\n",
    "\n",
    "    Keyword arguments:\n",
    "    directory -- the directory to list the csv files\n",
    "\n",
    "    Returns:\n",
    "    List[str] -- the list of csv files\n",
    "    \"\"\"\n",
    "    return [ f\"{directory}/{file}\" for file in os.listdir(directory)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    clean and collect the weather data hourly\n",
    "\n",
    "    Keyword arguments:\n",
    "    csv_file -- the csv file to be cleaned\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame -- the cleaned dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # parse the DATE and fetch the DATE_hour column and select the required columns\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df['DATE'] =  pd.to_datetime(df['DATE'])\n",
    "        df['DATE_hour'] = df['DATE'].dt.strftime('%Y-%m-%d-%H')\n",
    "        df = df.drop_duplicates(subset='DATE_hour', keep='first')\n",
    "        df = df[['DATE_hour','HourlyPrecipitation','HourlyWindSpeed']]\n",
    "        # replace T with 0.01\n",
    "        df['HourlyPrecipitation'] = df['HourlyPrecipitation'].replace('T', 0.00001)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    clean and collect the weather data daily\n",
    "\n",
    "    Keyword arguments:\n",
    "    csv_file -- the csv file to be cleaned\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame -- the cleaned dataframe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # using the last record of the day to represent the weather of the day\n",
    "        df = pd.read_csv(csv_file)\n",
    "        # df = df[df['DailyWeather'].notna()]#[['DailyWeather','DailyAverageDryBulbTemperature']]\n",
    "        df['DATE'] =  pd.to_datetime(df['DATE'])\n",
    "        df['DATE_day'] = df['DATE'].dt.strftime('%Y-%m-%d')\n",
    "        df['DailyPrecipitation'] = df['DailyPrecipitation'].replace('T', 0.00001).astype(float)\n",
    "        df['DailySnowfall'] = df['DailySnowfall'].replace('T', 0.00001).astype(float)\n",
    "        df[ 'DailyAverageWindSpeed'] =  df[ 'DailyAverageWindSpeed'].astype(float)\n",
    "        # Fill missing values in specified columns with their respective column means\n",
    "        df[['DailyPrecipitation', 'DailySnowfall', 'DailyAverageWindSpeed']] = (\n",
    "            df[['DailyPrecipitation', 'DailySnowfall', 'DailyAverageWindSpeed']].apply(\n",
    "                lambda col: col.fillna(col.mean() if col.mean() is not None else 0)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        df = df.drop_duplicates(subset='DATE_day', keep='last')\n",
    "        df = df[['DATE_day','DailyPrecipitation','DailySnowfall','DailyAverageWindSpeed']]\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(csv_file)\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data() -> Tuple[pd.DataFrame,pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    load and clean the weather data\n",
    "\n",
    "    Returns:\n",
    "    Tuple[pd.DataFrame,pd.DataFrame] -- the cleaned hourly and daily weather data    \n",
    "    \"\"\"\n",
    "\n",
    "    weather_csv_files = get_all_weather_csvs(WEATHER_CSV_DIR)\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "        \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(pd.read_csv('weather_data/2020_weather.csv').columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "\n",
    "DROP TABLE IF EXISTS hourly_weather;\n",
    "CREATE TABLE hourly_weather  (\n",
    "    DATE_hour TEXT PRIMARY KEY,\n",
    "    HourlyPrecipitation REAL,\n",
    "    HourlyWindSpeed REAL\n",
    ");\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "\n",
    "DROP TABLE IF EXISTS daily_weather;\n",
    "CREATE TABLE daily_weather (\n",
    "    DATE_day TEXT PRIMARY KEY,\n",
    "    DailyPrecipitation REAL,\n",
    "    DailySnowfall REAL,\n",
    "    DailyAverageWindSpeed REAL\n",
    ");\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "\n",
    "\n",
    "DROP TABLE IF EXISTS taxi_trips;\n",
    "CREATE TABLE taxi_trips (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    trip_pickup_datetime TEXT,\n",
    "    trip_dropoff_datetime TEXT,\n",
    "    trip_miles REAL,\n",
    "    base_fare REAL,\n",
    "    tax REAL,\n",
    "    tolls REAL,\n",
    "    surcharge REAL,\n",
    "    pickup_latitude REAL,\n",
    "    pickup_longitude REAL,\n",
    "    dropoff_latitude REAL,\n",
    "    dropoff_longitude REAL\n",
    ");\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "\n",
    "DROP TABLE IF EXISTS uber_trips;\n",
    "CREATE TABLE uber_trips (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    trip_pickup_datetime TEXT,\n",
    "    trip_dropoff_datetime TEXT,\n",
    "    trip_distance REAL,\n",
    "    base_fare REAL,\n",
    "    tax REAL,\n",
    "    tolls REAL,\n",
    "    surcharge REAL,\n",
    "    pickup_latitude REAL,\n",
    "    pickup_longitude REAL,\n",
    "    dropoff_latitude REAL,\n",
    "    dropoff_longitude REAL\n",
    ");\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATABASE_SCHEMA_FILE, \"r\") as f:\n",
    "    sql_script = f.read()\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    for statement in sql_script.split(\";\"):\n",
    "        statement = statement.strip()\n",
    "        if statement:  # Skip empty statements\n",
    "            connection.execute(text(statement))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict:Dict[str,pd.DataFrame]):\n",
    "    \"\"\"\n",
    "    write the dataframes to the tables in the database\n",
    "\n",
    "    Keyword arguments:\n",
    "    table_to_df_dict -- the dictionary of table name to dataframe mapping\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    for k,v in table_to_df_dict.items():\n",
    "        v.to_sql(k, con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data_cleaned,\n",
    "    \"uber_trips\": uber_data_cleaned,\n",
    "    \"hourly_weather\": hourly_weather_data,\n",
    "    \"daily_weather\": daily_weather_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query:str, outfile:str):\n",
    "    with open(f\"{QUERY_DIRECTORY}/{outfile}\", \"w\") as f:\n",
    "        f.write(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"query1.sql\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "SELECT \n",
    "    strftime('%H', trip_pickup_datetime) AS pickup_hour, \n",
    "    COUNT(*) AS trip_count\n",
    "FROM \n",
    "    taxi_trips\n",
    "GROUP BY \n",
    "    pickup_hour\n",
    "ORDER BY \n",
    "    trip_count DESC\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query either via sqlalchemy\n",
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_1)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "results1_df = pd.read_sql(QUERY_1, con=engine)\n",
    "results1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_2_FILENAME = \"query2.sql\"\n",
    "\n",
    "QUERY_2 = \"\"\"\n",
    "SELECT \n",
    "    strftime('%w', trip_pickup_datetime) AS day_of_week, \n",
    "    CASE strftime('%w', trip_pickup_datetime)\n",
    "        WHEN '0' THEN 'Sunday'\n",
    "        WHEN '1' THEN 'Monday'\n",
    "        WHEN '2' THEN 'Tuesday'\n",
    "        WHEN '3' THEN 'Wednesday'\n",
    "        WHEN '4' THEN 'Thursday'\n",
    "        WHEN '5' THEN 'Friday'\n",
    "        WHEN '6' THEN 'Saturday'\n",
    "    END AS day_of_week_name,\n",
    "    COUNT(*) AS trip_count\n",
    "FROM \n",
    "    uber_trips\n",
    "GROUP BY \n",
    "    day_of_week_name\n",
    "ORDER BY \n",
    "    trip_count DESC\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query either via sqlalchemy\n",
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_2)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "results2_df = pd.read_sql(QUERY_2, con=engine)\n",
    "results2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_2, QUERY_2_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_3_FILENAME = \"query3.sql\"\n",
    "\n",
    "QUERY_3 = \"\"\"\n",
    "WITH combined_trips AS (\n",
    "    SELECT \n",
    "        trip_miles AS trip_distance,\n",
    "        trip_pickup_datetime\n",
    "    FROM \n",
    "        taxi_trips\n",
    "    WHERE \n",
    "        strftime('%Y-%m', trip_pickup_datetime) = '2024-01'\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        trip_distance AS trip_distance,\n",
    "        trip_pickup_datetime\n",
    "    FROM \n",
    "        uber_trips\n",
    "    WHERE \n",
    "        strftime('%Y-%m', trip_pickup_datetime) = '2024-01'\n",
    "),\n",
    "sorted_trips AS (\n",
    "    SELECT \n",
    "        trip_distance,\n",
    "        ROW_NUMBER() OVER (ORDER BY trip_distance) AS row_num,\n",
    "        COUNT(*) OVER () AS total_rows\n",
    "    FROM \n",
    "        combined_trips\n",
    "),\n",
    "percentile_row AS (\n",
    "    SELECT \n",
    "        trip_distance\n",
    "    FROM \n",
    "        sorted_trips\n",
    "    WHERE \n",
    "        row_num = CAST(0.95 * total_rows AS INTEGER)\n",
    ")\n",
    "SELECT \n",
    "    trip_distance AS percentile_95\n",
    "FROM \n",
    "    percentile_row;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query either via sqlalchemy\n",
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_3)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "results3_df = pd.read_sql(QUERY_3, con=engine)\n",
    "results3_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_3, QUERY_3_FILENAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
