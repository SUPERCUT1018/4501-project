{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones(shapefile:str='./taxi_zones.shp') -> gpd.GeoDataFrame :\n",
    "    \"\"\"\n",
    "    Load the taxi zone shapefile into a GeoDataFrame\n",
    "\n",
    "    Keyword Arguments:\n",
    "    shapefile {str} -- the path to the shapefile (default: {'./taxi_zones.shp'})\n",
    "\n",
    "    Returns:\n",
    "    gpd.GeoDataFrame -- the GeoDataFrame containing the taxi zones\n",
    "    \"\"\"\n",
    "    gdf = gpd.read_file(shapefile).to_crs(4326)\n",
    "    gdf['latitude'] = gdf.geometry.centroid.y\n",
    "    gdf['longitude'] = gdf.geometry.centroid.x\n",
    "    gdf = gdf[['LocationID', 'latitude', 'longitude']]\n",
    "    return gdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = load_taxi_zones()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sample_size(population,z=1.96, e=0.05) -> int:\n",
    "    \"\"\"\n",
    "    calculate the sample size needed for a given population\n",
    "    :param population: the size of the population\n",
    "    :param z: the parameter of confidence level\n",
    "    :param e: the margin of error\n",
    "\n",
    "    :return: the sample size needed\n",
    "    \"\"\"\n",
    "    n0 = z**2 * 0.5 * 0.5 / e**2\n",
    "    n = n0 / (1 + (n0 - 1) / population)\n",
    "    return int(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_location_id_to_lat_lon(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    convert the pickup and dropoff location id to latitude and longitude\n",
    "\n",
    "    Keyword arguments:\n",
    "    df -- the dataframe to be converted\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame -- the dataframe with latitude and longitude columns\n",
    "    \"\"\"\n",
    "    df = df.merge(gdf, left_on='PULocationID', right_on='LocationID', how='left').rename(columns={'latitude':'PULatitude', 'longitude':'PULongitude'}).drop(columns='LocationID')\n",
    "    df = df.dropna(subset=['PULatitude', 'PULongitude'])\n",
    "    df = df.merge(gdf, left_on='DOLocationID', right_on='LocationID', how='left').rename(columns={'latitude':'DOLatitude', 'longitude':'DOLongitude'}).drop(columns='LocationID')\n",
    "    df = df.dropna(subset=['DOLatitude', 'DOLongitude'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_taxi_urls(all_urls:List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    filter the urls that contain the yellow taxi data\n",
    "\n",
    "    Keyword arguments:\n",
    "    all_urls -- the list of urls to be filtered\n",
    "\n",
    "    Returns:\n",
    "    List[str] -- the list of filtered urls\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'(yellow)_tripdata_.*?(202[0-4])-([0-1][0-9])')\n",
    "    result = []\n",
    "    for url in all_urls:\n",
    "        filename = url.split(\"/\")[-1]\n",
    "        match = pattern.match(filename)\n",
    "        if match is not None:\n",
    "            result.append(url)\n",
    "    return result\n",
    "\n",
    "\n",
    "def filter_urls(all_urls:List[str],re_pattern :str= '(fhvhv)_tripdata_.*?(202[0-4])-([0-1][0-9])') -> List[str]:\n",
    "    \"\"\"\n",
    "    filter url with given pattern\n",
    "\n",
    "    Keyword arguments:\n",
    "    all_urls -- the list of urls to be filtered\n",
    "    re_pattern -- the pattern to be matched\n",
    "\n",
    "    Returns:\n",
    "    List[str] -- the list of filtered urls\n",
    "    \"\"\"\n",
    "    pattern = re.compile(f\"{re_pattern}\")\n",
    "    result = []\n",
    "    for url in all_urls:\n",
    "        filename = url.split(\"/\")[-1]\n",
    "        match = pattern.match(filename)\n",
    "        if match is not None:\n",
    "            result.append(url)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid_record_of_taxi_data(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    remove the data points outside of NYC\n",
    "    remove the data with a nan pick\n",
    "    remove the distance less equal than 0\n",
    "    \n",
    "    Keyword arguments:\n",
    "\n",
    "    Returns:\n",
    "    pd.Dataframe -- the row if it is valid, None otherwise\n",
    "    \"\"\"\n",
    "    valid = (\n",
    "            (df['PULatitude'].between(NEW_YORK_BOX_COORDS[0][0], NEW_YORK_BOX_COORDS[1][0])) &\n",
    "            (df['PULongitude'].between(NEW_YORK_BOX_COORDS[0][1], NEW_YORK_BOX_COORDS[1][1])) &\n",
    "            (df['DOLatitude'].between(NEW_YORK_BOX_COORDS[0][0], NEW_YORK_BOX_COORDS[1][0])) &\n",
    "            (df['DOLongitude'].between(NEW_YORK_BOX_COORDS[0][1], NEW_YORK_BOX_COORDS[1][1])) &\n",
    "            (~df['tpep_pickup_datetime'].isna()) &\n",
    "            (~df['tpep_dropoff_datetime'].isna()) &\n",
    "            (df['trip_distance'] > 0)\n",
    "        )\n",
    "    \n",
    "    return df[valid]\n",
    "\n",
    "def get_and_clean_month(url:str)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    download and clean the data from the given url\n",
    "\n",
    "    Keyword arguements:\n",
    "    url -- the url to download the data\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame -- the cleaned dataframe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parquet_file = f\"{url.split('/')[-1].strip()}\"\n",
    "        if os.path.exists(f\"{PARQUET_DIR}/{parquet_file}\"):\n",
    "            df = pd.read_parquet(f\"{PARQUET_DIR}/{parquet_file}\")\n",
    "        else:\n",
    "            # wget = f\"wget {url.strip()} -O {PARQUET_DIR}/{parquet_file}\"\n",
    "            # os.system(wget)\n",
    "\n",
    "            response = requests.get(url.strip(), stream=True)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "            file_path = os.path.join(PARQUET_DIR, parquet_file)\n",
    "            # Save the content to the file\n",
    "            with open(file_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            df = pd.read_parquet(f\"{PARQUET_DIR}/{parquet_file}\")\n",
    "            \n",
    "        n = calculate_sample_size(df.shape[0])\n",
    "        df = df.sample(n)\n",
    "        df = convert_location_id_to_lat_lon(df)\n",
    "        df = remove_invalid_record_of_taxi_data(df)\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(parquet_urls:List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    download and clean the data from the given urls\n",
    "\n",
    "    Keyword arguements:\n",
    "    parquet_urls -- the list of urls to download the data\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame -- the cleaned dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    all_taxi_dataframes = []\n",
    "    parquet_urls = filter_urls(parquet_urls,'(yellow)_tripdata_.*?(202[0-4])-([0-1][0-9])')\n",
    "    for parquet_url in tqdm(parquet_urls):\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_month(parquet_url)\n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        \n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_taxi_page(url:str) -> List[str]:\n",
    "    \"\"\"\n",
    "    get all urls from the page of the given url\n",
    "\n",
    "    Keyword arguments:\n",
    "    url -- the url to get all urls from\n",
    "\n",
    "    Returns:\n",
    "    List[str] -- the list of all urls\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = bs4.BeautifulSoup(response.content, 'html.parser')\n",
    "    urls = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "    return urls\n",
    "\n",
    "\n",
    "def find_all_parquet_urls(urls:List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    filter the all parquet urls \n",
    "\n",
    "    Keyword arguments:\n",
    "    urls -- the list of urls to be filtered\n",
    "\n",
    "    Returns:\n",
    "    List[str] -- the list of filtered urls\n",
    "    \"\"\"\n",
    "    parquet_urls = [url for url in urls if 'parquet' in url]\n",
    "    return parquet_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data() ->pd.DataFrame:\n",
    "    \"\"\" \n",
    "    get the taxi data from the TLC website\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame -- the taxi data\n",
    "    \"\"\"\n",
    "    if not os.path.exists(PARQUET_DIR):\n",
    "        os.mkdir(PARQUET_DIR)\n",
    "    all_urls = get_all_urls_from_taxi_page(TLC_URL)\n",
    "    all_parquet_urls = find_all_parquet_urls(all_urls)\n",
    "    taxi_data = get_and_clean_taxi_data(all_parquet_urls)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data_uncleaned = get_taxi_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data_cleaned = taxi_data_uncleaned[['tpep_pickup_datetime','tpep_dropoff_datetime',\n",
    "                               'trip_distance','fare_amount','extra','mta_tax','improvement_surcharge',\n",
    "                               'tolls_amount',\n",
    "                               'PULatitude','PULongitude',\n",
    "                               'DOLatitude','DOLongitude']]\n",
    "\n",
    "taxi_data_cleaned['base_fare'] = taxi_data_cleaned['fare_amount'] \n",
    "taxi_data_cleaned['tax'] = taxi_data_cleaned['mta_tax'] + taxi_data_cleaned['extra']\n",
    "taxi_data_cleaned['tolls'] = taxi_data_cleaned['tolls_amount']\n",
    "taxi_data_cleaned['surcharge'] = taxi_data_cleaned['improvement_surcharge']\n",
    "taxi_data_cleaned = taxi_data_cleaned.rename(columns={'tpep_pickup_datetime':'trip_pickup_datetime',\n",
    "                                                      'tpep_dropoff_datetime':'trip_dropoff_datetime',\n",
    "                                                      'trip_distance':'trip_miles',\n",
    "                                                      'base_fare':'base_fare',\n",
    "                                                      'tax':'tax',\n",
    "                                                      'tolls':'tolls',\n",
    "                                                      'PULatitude':'pickup_latitude',\n",
    "                                                      'PULongitude':'pickup_longitude',\n",
    "                                                      'DOLatitude':'dropoff_latitude',\n",
    "                                                      'DOLongitude':'dropoff_longitude'})\n",
    "taxi_data_cleaned = taxi_data_cleaned[['trip_pickup_datetime','trip_dropoff_datetime','trip_miles','base_fare','tax','tolls','surcharge',\n",
    "                                       'pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude']]\n",
    "taxi_data = taxi_data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid_records_of_uber_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove invalid data points from the Uber dataset.\n",
    "\n",
    "    Keyword arguments:\n",
    "    df -- the DataFrame to clean\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame -- the cleaned DataFrame\n",
    "    \"\"\"\n",
    "    valid = (\n",
    "        (df['PULatitude'].between(NEW_YORK_BOX_COORDS[0][0], NEW_YORK_BOX_COORDS[1][0])) &\n",
    "        (df['PULongitude'].between(NEW_YORK_BOX_COORDS[0][1], NEW_YORK_BOX_COORDS[1][1])) &\n",
    "        (df['DOLatitude'].between(NEW_YORK_BOX_COORDS[0][0], NEW_YORK_BOX_COORDS[1][0])) &\n",
    "        (df['DOLongitude'].between(NEW_YORK_BOX_COORDS[0][1], NEW_YORK_BOX_COORDS[1][1])) &\n",
    "        (~df['pickup_datetime'].isna()) &\n",
    "        (~df['dropoff_datetime'].isna()) &\n",
    "        (df['trip_miles'] > 0)\n",
    "    )\n",
    "\n",
    "    # Return the filtered DataFrame\n",
    "    return df[valid]\n",
    "\n",
    "\n",
    "def get_and_clean_uber_month(url:List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    download and clean the data from the given url of uber\n",
    "\n",
    "    Keyword arguements:\n",
    "    url -- the url to download the data\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame -- the cleaned dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        parquet_file = f\"{url.split('/')[-1].strip()}\"\n",
    "        if os.path.exists(f\"{PARQUET_DIR}/{parquet_file}\"):\n",
    "            df = pd.read_parquet(f\"{PARQUET_DIR}/{parquet_file}\")\n",
    "        else:\n",
    "            # wget = f\"wget {url.strip()} -O {PARQUET_DIR}/{parquet_file}\"\n",
    "            # os.system(wget)\n",
    "\n",
    "            response = requests.get(url.strip(), stream=True)\n",
    "            response.raise_for_status()\n",
    "            file_path = os.path.join(PARQUET_DIR, parquet_file)\n",
    "            with open(file_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            df = pd.read_parquet(f\"{PARQUET_DIR}/{parquet_file}\")\n",
    "            \n",
    "        n = calculate_sample_size(df.shape[0])\n",
    "        df = df[df['hvfhs_license_num']=='HV0003']\n",
    "        df = df.sample(n)\n",
    "        df = convert_location_id_to_lat_lon(df)\n",
    "        df = remove_invalid_records_of_uber_data(df)\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_uber_data(parquet_urls:List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    download and clean the data from the given urls of uber\n",
    "\n",
    "    Keyword arguements:\n",
    "    parquet_urls -- the list of urls to download the data\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame -- the cleaned dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    all_uber_dataframes = []\n",
    "\n",
    "    uber_parquet_urls = filter_urls(parquet_urls, re_pattern='(fhvhv)_tripdata_.*?(202[0-4])-([0-1][0-9])')\n",
    "\n",
    "    for parquet_url in tqdm(uber_parquet_urls):\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_uber_month(parquet_url)\n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        \n",
    "        all_uber_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    uber_data = pd.concat(all_uber_dataframes)\n",
    "    return uber_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data() -> pd.DataFrame :\n",
    "    \"\"\"\n",
    "    get the uber data from the TLC website\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame -- the uber data\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(PARQUET_DIR):\n",
    "        os.mkdir(PARQUET_DIR)\n",
    "    all_urls = get_all_urls_from_taxi_page(TLC_URL)\n",
    "    all_parquet_urls = find_all_parquet_urls(all_urls)\n",
    "    taxi_data = get_and_clean_uber_data(all_parquet_urls)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data_uncleaned = get_uber_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data_cleaned = uber_data_uncleaned[['pickup_datetime','dropoff_datetime','trip_miles',\n",
    "                               'base_passenger_fare','tolls','bcf','sales_tax','congestion_surcharge',\n",
    "                               'PULatitude','PULongitude',\n",
    "                               'DOLatitude','DOLongitude']]\n",
    "\n",
    "uber_data_cleaned['base_fare'] = uber_data_cleaned['base_passenger_fare'] #+ uber_data_cleaned['bcf']\n",
    "uber_data_cleaned['surcharge'] = uber_data_cleaned['congestion_surcharge']\n",
    "uber_data_cleaned['tax'] = uber_data_cleaned['sales_tax'] +   uber_data_cleaned['bcf'] # TODO: check if this is correct\n",
    "uber_data_cleaned['tolls'] = uber_data_cleaned['tolls']\n",
    "uber_data_cleaned = uber_data_cleaned.rename(columns={\n",
    "    'trip_miles':'trip_distance',\n",
    "    'pickup_datetime':'trip_pickup_datetime',\n",
    "    'dropoff_datetime':'trip_dropoff_datetime',\n",
    "    'base_fare':'base_fare',\n",
    "    'tax':'tax',\n",
    "    'tolls':'tolls',\n",
    "    'surcharge':'surcharge',\n",
    "    'PULatitude':'pickup_latitude',\n",
    "    'PULongitude':'pickup_longitude',\n",
    "    'DOLatitude':'dropoff_latitude',\n",
    "    'DOLongitude':'dropoff_longitude'\n",
    "})\n",
    "\n",
    "uber_data_cleaned = uber_data_cleaned[['trip_pickup_datetime','trip_dropoff_datetime','trip_distance','base_fare','tax','tolls','surcharge',\n",
    "                                       'pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude']]\n",
    "\n",
    "uber_data = uber_data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory:str) -> List[str]:\n",
    "    \"\"\"\n",
    "    list all the csv files in the given directory\n",
    "\n",
    "    Keyword arguments:\n",
    "    directory -- the directory to list the csv files\n",
    "\n",
    "    Returns:\n",
    "    List[str] -- the list of csv files\n",
    "    \"\"\"\n",
    "    return [ f\"{directory}/{file}\" for file in os.listdir(directory)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    clean and collect the weather data hourly\n",
    "\n",
    "    Keyword arguments:\n",
    "    csv_file -- the csv file to be cleaned\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame -- the cleaned dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # parse the DATE and fetch the DATE_hour column and select the required columns\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df['DATE'] =  pd.to_datetime(df['DATE'])\n",
    "        df['DATE_hour'] = df['DATE'].dt.strftime('%Y-%m-%d-%H')\n",
    "        df = df.drop_duplicates(subset='DATE_hour', keep='first')\n",
    "        df = df[['DATE_hour','HourlyPrecipitation','HourlyWindSpeed']]\n",
    "        # replace T with 0.01\n",
    "        df['HourlyPrecipitation'] = df['HourlyPrecipitation'].replace('T', 0.00001)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    clean and collect the weather data daily\n",
    "\n",
    "    Keyword arguments:\n",
    "    csv_file -- the csv file to be cleaned\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame -- the cleaned dataframe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # using the last record of the day to represent the weather of the day\n",
    "        df = pd.read_csv(csv_file)\n",
    "        # df = df[df['DailyWeather'].notna()]#[['DailyWeather','DailyAverageDryBulbTemperature']]\n",
    "        df['DATE'] =  pd.to_datetime(df['DATE'])\n",
    "        df['DATE_day'] = df['DATE'].dt.strftime('%Y-%m-%d')\n",
    "        df['DailyPrecipitation'] = df['DailyPrecipitation'].replace('T', 0.00001).astype(float)\n",
    "        df['DailySnowfall'] = df['DailySnowfall'].replace('T', 0.00001).astype(float)\n",
    "        df[ 'DailyAverageWindSpeed'] =  df[ 'DailyAverageWindSpeed'].astype(float)\n",
    "        # Fill missing values in specified columns with their respective column means\n",
    "        df[['DailyPrecipitation', 'DailySnowfall', 'DailyAverageWindSpeed']] = (\n",
    "            df[['DailyPrecipitation', 'DailySnowfall', 'DailyAverageWindSpeed']].apply(\n",
    "                lambda col: col.fillna(col.mean() if col.mean() is not None else 0)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        df = df.drop_duplicates(subset='DATE_day', keep='last')\n",
    "        df = df[['DATE_day','DailyPrecipitation','DailySnowfall','DailyAverageWindSpeed']]\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(csv_file)\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data() -> Tuple[pd.DataFrame,pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    load and clean the weather data\n",
    "\n",
    "    Returns:\n",
    "    Tuple[pd.DataFrame,pd.DataFrame] -- the cleaned hourly and daily weather data    \n",
    "    \"\"\"\n",
    "\n",
    "    weather_csv_files = get_all_weather_csvs(WEATHER_CSV_DIR)\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "        \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(pd.read_csv('weather_data/2020_weather.csv').columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.describe()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
