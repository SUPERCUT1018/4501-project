{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones(shapefile:str='./taxi_zones.shp') -> gpd.GeoDataFrame :\n",
    "    \"\"\"\n",
    "    Load the taxi zone shapefile into a GeoDataFrame\n",
    "\n",
    "    Keyword Arguments:\n",
    "    shapefile {str} -- the path to the shapefile (default: {'./taxi_zones.shp'})\n",
    "\n",
    "    Returns:\n",
    "    gpd.GeoDataFrame -- the GeoDataFrame containing the taxi zones\n",
    "    \"\"\"\n",
    "    gdf = gpd.read_file(shapefile).to_crs(4326)\n",
    "    gdf['latitude'] = gdf.geometry.centroid.y\n",
    "    gdf['longitude'] = gdf.geometry.centroid.x\n",
    "    gdf = gdf[['LocationID', 'latitude', 'longitude']]\n",
    "    return gdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = load_taxi_zones()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sample_size(population,z=1.96, e=0.05) -> int:\n",
    "    \"\"\"\n",
    "    calculate the sample size needed for a given population\n",
    "    :param population: the size of the population\n",
    "    :param z: the parameter of confidence level\n",
    "    :param e: the margin of error\n",
    "\n",
    "    :return: the sample size needed\n",
    "    \"\"\"\n",
    "    n0 = z**2 * 0.5 * 0.5 / e**2\n",
    "    n = n0 / (1 + (n0 - 1) / population)\n",
    "    return int(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_location_id_to_lat_lon(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    convert the pickup and dropoff location id to latitude and longitude\n",
    "\n",
    "    Keyword arguments:\n",
    "    df -- the dataframe to be converted\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame -- the dataframe with latitude and longitude columns\n",
    "    \"\"\"\n",
    "    df = df.merge(gdf, left_on='PULocationID', right_on='LocationID', how='left').rename(columns={'latitude':'PULatitude', 'longitude':'PULongitude'}).drop(columns='LocationID')\n",
    "    df = df.dropna(subset=['PULatitude', 'PULongitude'])\n",
    "    df = df.merge(gdf, left_on='DOLocationID', right_on='LocationID', how='left').rename(columns={'latitude':'DOLatitude', 'longitude':'DOLongitude'}).drop(columns='LocationID')\n",
    "    df = df.dropna(subset=['DOLatitude', 'DOLongitude'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_taxi_urls(all_urls:List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    filter the urls that contain the yellow taxi data\n",
    "\n",
    "    Keyword arguments:\n",
    "    all_urls -- the list of urls to be filtered\n",
    "\n",
    "    Returns:\n",
    "    List[str] -- the list of filtered urls\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'(yellow)_tripdata_.*?(202[0-4])-([0-1][0-9])')\n",
    "    result = []\n",
    "    for url in all_urls:\n",
    "        filename = url.split(\"/\")[-1]\n",
    "        match = pattern.match(filename)\n",
    "        if match is not None:\n",
    "            result.append(url)\n",
    "    return result\n",
    "\n",
    "\n",
    "def filter_urls(all_urls:List[str],re_pattern :str= '(fhvhv)_tripdata_.*?(202[0-4])-([0-1][0-9])') -> List[str]:\n",
    "    \"\"\"\n",
    "    filter url with given pattern\n",
    "\n",
    "    Keyword arguments:\n",
    "    all_urls -- the list of urls to be filtered\n",
    "    re_pattern -- the pattern to be matched\n",
    "\n",
    "    Returns:\n",
    "    List[str] -- the list of filtered urls\n",
    "    \"\"\"\n",
    "    pattern = re.compile(f\"{re_pattern}\")\n",
    "    result = []\n",
    "    for url in all_urls:\n",
    "        filename = url.split(\"/\")[-1]\n",
    "        match = pattern.match(filename)\n",
    "        if match is not None:\n",
    "            result.append(url)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid_record_of_taxi_data(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    remove the data points outside of NYC\n",
    "    remove the data with a nan pick\n",
    "    remove the distance less equal than 0\n",
    "    \n",
    "    Keyword arguments:\n",
    "\n",
    "    Returns:\n",
    "    pd.Dataframe -- the row if it is valid, None otherwise\n",
    "    \"\"\"\n",
    "    valid = (\n",
    "            (df['PULatitude'].between(NEW_YORK_BOX_COORDS[0][0], NEW_YORK_BOX_COORDS[1][0])) &\n",
    "            (df['PULongitude'].between(NEW_YORK_BOX_COORDS[0][1], NEW_YORK_BOX_COORDS[1][1])) &\n",
    "            (df['DOLatitude'].between(NEW_YORK_BOX_COORDS[0][0], NEW_YORK_BOX_COORDS[1][0])) &\n",
    "            (df['DOLongitude'].between(NEW_YORK_BOX_COORDS[0][1], NEW_YORK_BOX_COORDS[1][1])) &\n",
    "            (~df['tpep_pickup_datetime'].isna()) &\n",
    "            (~df['tpep_dropoff_datetime'].isna()) &\n",
    "            (df['trip_distance'] > 0)\n",
    "        )\n",
    "    \n",
    "    return df[valid]\n",
    "\n",
    "def get_and_clean_month(url:str)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    download and clean the data from the given url\n",
    "\n",
    "    Keyword arguements:\n",
    "    url -- the url to download the data\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame -- the cleaned dataframe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parquet_file = f\"{url.split('/')[-1].strip()}\"\n",
    "        if os.path.exists(f\"{PARQUET_DIR}/{parquet_file}\"):\n",
    "            df = pd.read_parquet(f\"{PARQUET_DIR}/{parquet_file}\")\n",
    "        else:\n",
    "            # wget = f\"wget {url.strip()} -O {PARQUET_DIR}/{parquet_file}\"\n",
    "            # os.system(wget)\n",
    "\n",
    "            response = requests.get(url.strip(), stream=True)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "            file_path = os.path.join(PARQUET_DIR, parquet_file)\n",
    "            # Save the content to the file\n",
    "            with open(file_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            df = pd.read_parquet(f\"{PARQUET_DIR}/{parquet_file}\")\n",
    "            \n",
    "        n = calculate_sample_size(df.shape[0])\n",
    "        df = df.sample(n)\n",
    "        df = convert_location_id_to_lat_lon(df)\n",
    "        df = remove_invalid_record_of_taxi_data(df)\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(parquet_urls:List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    download and clean the data from the given urls\n",
    "\n",
    "    Keyword arguements:\n",
    "    parquet_urls -- the list of urls to download the data\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame -- the cleaned dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    all_taxi_dataframes = []\n",
    "    parquet_urls = filter_urls(parquet_urls,'(yellow)_tripdata_.*?(202[0-4])-([0-1][0-9])')\n",
    "    for parquet_url in tqdm(parquet_urls):\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_month(parquet_url)\n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        \n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_taxi_page(url:str) -> List[str]:\n",
    "    \"\"\"\n",
    "    get all urls from the page of the given url\n",
    "\n",
    "    Keyword arguments:\n",
    "    url -- the url to get all urls from\n",
    "\n",
    "    Returns:\n",
    "    List[str] -- the list of all urls\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = bs4.BeautifulSoup(response.content, 'html.parser')\n",
    "    urls = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "    return urls\n",
    "\n",
    "\n",
    "def find_all_parquet_urls(urls:List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    filter the all parquet urls \n",
    "\n",
    "    Keyword arguments:\n",
    "    urls -- the list of urls to be filtered\n",
    "\n",
    "    Returns:\n",
    "    List[str] -- the list of filtered urls\n",
    "    \"\"\"\n",
    "    parquet_urls = [url for url in urls if 'parquet' in url]\n",
    "    return parquet_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data() ->pd.DataFrame:\n",
    "    \"\"\" \n",
    "    get the taxi data from the TLC website\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame -- the taxi data\n",
    "    \"\"\"\n",
    "    if not os.path.exists(PARQUET_DIR):\n",
    "        os.mkdir(PARQUET_DIR)\n",
    "    all_urls = get_all_urls_from_taxi_page(TLC_URL)\n",
    "    all_parquet_urls = find_all_parquet_urls(all_urls)\n",
    "    taxi_data = get_and_clean_taxi_data(all_parquet_urls)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data_uncleaned = get_taxi_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data_cleaned = taxi_data_uncleaned[['tpep_pickup_datetime','tpep_dropoff_datetime',\n",
    "                               'trip_distance','fare_amount','extra','mta_tax','improvement_surcharge',\n",
    "                               'tolls_amount',\n",
    "                               'PULatitude','PULongitude',\n",
    "                               'DOLatitude','DOLongitude']]\n",
    "\n",
    "taxi_data_cleaned['base_fare'] = taxi_data_cleaned['fare_amount'] \n",
    "taxi_data_cleaned['tax'] = taxi_data_cleaned['mta_tax'] + taxi_data_cleaned['extra']\n",
    "taxi_data_cleaned['tolls'] = taxi_data_cleaned['tolls_amount']\n",
    "taxi_data_cleaned['surcharge'] = taxi_data_cleaned['improvement_surcharge']\n",
    "taxi_data_cleaned = taxi_data_cleaned.rename(columns={'tpep_pickup_datetime':'trip_pickup_datetime',\n",
    "                                                      'tpep_dropoff_datetime':'trip_dropoff_datetime',\n",
    "                                                      'trip_distance':'trip_miles',\n",
    "                                                      'base_fare':'base_fare',\n",
    "                                                      'tax':'tax',\n",
    "                                                      'tolls':'tolls',\n",
    "                                                      'PULatitude':'pickup_latitude',\n",
    "                                                      'PULongitude':'pickup_longitude',\n",
    "                                                      'DOLatitude':'dropoff_latitude',\n",
    "                                                      'DOLongitude':'dropoff_longitude'})\n",
    "taxi_data_cleaned = taxi_data_cleaned[['trip_pickup_datetime','trip_dropoff_datetime','trip_miles','base_fare','tax','tolls','surcharge',\n",
    "                                       'pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude']]\n",
    "taxi_data = taxi_data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.describe()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
